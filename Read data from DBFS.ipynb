{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa98fda-a7d8-4253-8afa-69f9fb723e2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs\n",
    "from pyspark.sql.types import StringType, DoubleType, StructField, StructType, TimestampType, ArrayType, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, col, desc, explode, when, trim\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df13444-ea0e-4dc2-b151-a26643977da9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_files_to_bronze():\n",
    "    json_directory = \"dbfs:/FileStore/tables/bronze/\"\n",
    "\n",
    "    movie_schema = StructType([\n",
    "    StructField(\"Id\", LongType(), True),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"Overview\", StringType(), True),\n",
    "    StructField(\"Tagline\", StringType(), True),\n",
    "    StructField(\"Budget\", DoubleType(), True),\n",
    "    StructField(\"Revenue\", DoubleType(), True),\n",
    "    StructField(\"ImdbUrl\", StringType(), True),\n",
    "    StructField(\"TmdbUrl\", StringType(), True),\n",
    "    StructField(\"PosterUrl\", StringType(), True),\n",
    "    StructField(\"BackdropUrl\", StringType(), True),\n",
    "    StructField(\"OriginalLanguage\", StringType(), True),\n",
    "    StructField(\"ReleaseDate\", StringType(), True),\n",
    "    StructField(\"RunTime\", LongType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "    StructField(\"CreatedDate\", StringType(), True),\n",
    "    StructField(\"UpdatedDate\", StringType(), True),\n",
    "    StructField(\"UpdatedBy\", StringType(), True),\n",
    "    StructField(\"CreatedBy\", StringType(), True),\n",
    "    StructField(\"genres\", ArrayType(StructType([\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "        ])), True)\n",
    "    ])\n",
    "\n",
    "    root_schema = StructType([\n",
    "        StructField(\"movie\", ArrayType(movie_schema), True)\n",
    "    ])\n",
    "    \n",
    "    movie_df = (spark.read\n",
    "            .option(\"multiline\",\"true\")\n",
    "            .option(\"schema\", \"root_schema\")\n",
    "            #  .option(\"inferSchema\",\"true\")\n",
    "            .json(json_directory + \"*.json\")\n",
    "    )\n",
    "\n",
    "    # flatten json file\n",
    "    bronze_df = (movie_df.select(explode(col(\"movie\")).alias(\"movie\"))\n",
    "                    .select(\"movie.Id\", \"movie.Title\", \"movie.Overview\", \"movie.Tagline\", \"movie.Budget\", \"movie.Revenue\",\n",
    "                            \"movie.ImdbUrl\", \"movie.TmdbUrl\", \"movie.PosterUrl\", \"movie.BackdropUrl\",\n",
    "                            \"movie.OriginalLanguage\", \n",
    "                            to_timestamp(\"movie.ReleaseDate\", \"yyyy-MM-dd'T'HH:mm:ss\").alias(\"ReleaseDate\"),\n",
    "                            \"movie.RunTime\", \"movie.Price\",\n",
    "                            to_timestamp(\"movie.CreatedDate\", \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSS\").alias(\"CreatedDate\"),\n",
    "                            \"movie.UpdatedDate\", \"movie.UpdatedBy\", \"movie.CreatedBy\",\n",
    "                            explode(\"movie.genres\").alias(\"genre\"))\n",
    "                    .select(\"Id\", \"Title\", \"Overview\", \"Tagline\", \"Budget\", \"Revenue\", \"ImdbUrl\", \"TmdbUrl\",\n",
    "                            \"PosterUrl\", \"BackdropUrl\", \"OriginalLanguage\", \"ReleaseDate\", \"RunTime\",\n",
    "                            \"Price\", \"CreatedDate\",col(\"genre.id\").alias(\"genre_id\"), \"genre.name\"))\n",
    "    return bronze_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cccdb26-577f-4155-99dc-0829a77fb15f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1569639608351509>, line 45\u001B[0m\n",
       "\u001B[1;32m     42\u001B[0m     \u001B[38;5;66;03m# write movie table to silver\u001B[39;00m\n",
       "\u001B[1;32m     43\u001B[0m     silver_movies_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwriteSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/silver/movies\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 45\u001B[0m process_bronze_to_silver()\n",
       "\n",
       "File \u001B[0;32m<command-1569639608351509>, line 6\u001B[0m, in \u001B[0;36mprocess_bronze_to_silver\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m bronze_df \u001B[38;5;241m=\u001B[39m bronze_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquarantined\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRunTime\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m quarantined_df \u001B[38;5;241m=\u001B[39m bronze_df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquarantined\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[0;32m----> 6\u001B[0m \u001B[43mquarantined_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbfs:/FileStore/tables/bronze/quarantine/area\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m clean_df \u001B[38;5;241m=\u001B[39m bronze_df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquarantined\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[1;32m      9\u001B[0m clean_df \u001B[38;5;241m=\u001B[39m clean_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBudget\u001B[39m\u001B[38;5;124m\"\u001B[39m, when(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBudget\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1000000.0\u001B[39m, \u001B[38;5;241m1000000.0\u001B[39m)\u001B[38;5;241m.\u001B[39motherwise(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBudget\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1718\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1716\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1717\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1718\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: A schema mismatch detected when writing to the Delta table (Table ID: bf9b66cb-5e6b-4666-a93c-a17289b2c260).\n",
       "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
       "'.option(\"mergeSchema\", \"true\")'.\n",
       "For other operations, set the session configuration\n",
       "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
       "specific to the operation for details.\n",
       "\n",
       "Table schema:\n",
       "root\n",
       "-- Id: long (nullable = true)\n",
       "-- Title: string (nullable = true)\n",
       "-- Overview: string (nullable = true)\n",
       "-- Budget: double (nullable = true)\n",
       "-- Revenue: double (nullable = true)\n",
       "-- ImdbUrl: string (nullable = true)\n",
       "-- TmdbUrl: string (nullable = true)\n",
       "-- PosterUrl: string (nullable = true)\n",
       "-- BackdropUrl: string (nullable = true)\n",
       "-- OriginalLanguage: string (nullable = true)\n",
       "-- ReleaseDate: timestamp (nullable = true)\n",
       "-- RunTime: long (nullable = true)\n",
       "-- Price: double (nullable = true)\n",
       "-- CreatedDate: timestamp (nullable = true)\n",
       "-- genre_id: long (nullable = true)\n",
       "-- name: string (nullable = true)\n",
       "-- quarantined: boolean (nullable = true)\n",
       "\n",
       "\n",
       "Data schema:\n",
       "root\n",
       "-- Id: long (nullable = true)\n",
       "-- Title: string (nullable = true)\n",
       "-- Overview: string (nullable = true)\n",
       "-- Tagline: string (nullable = true)\n",
       "-- Budget: double (nullable = true)\n",
       "-- Revenue: double (nullable = true)\n",
       "-- ImdbUrl: string (nullable = true)\n",
       "-- TmdbUrl: string (nullable = true)\n",
       "-- PosterUrl: string (nullable = true)\n",
       "-- BackdropUrl: string (nullable = true)\n",
       "-- OriginalLanguage: string (nullable = true)\n",
       "-- ReleaseDate: timestamp (nullable = true)\n",
       "-- RunTime: long (nullable = true)\n",
       "-- Price: double (nullable = true)\n",
       "-- CreatedDate: timestamp (nullable = true)\n",
       "-- genre_id: long (nullable = true)\n",
       "-- name: string (nullable = true)\n",
       "-- quarantined: boolean (nullable = true)\n",
       "\n",
       "         \n",
       "To overwrite your schema or change partitioning, please set:\n",
       "'.option(\"overwriteSchema\", \"true\")'.\n",
       "\n",
       "Note that the schema can't be overwritten when using\n",
       "'replaceWhere'.\n",
       "         "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: bf9b66cb-5e6b-4666-a93c-a17289b2c260).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- Id: long (nullable = true)\n-- Title: string (nullable = true)\n-- Overview: string (nullable = true)\n-- Budget: double (nullable = true)\n-- Revenue: double (nullable = true)\n-- ImdbUrl: string (nullable = true)\n-- TmdbUrl: string (nullable = true)\n-- PosterUrl: string (nullable = true)\n-- BackdropUrl: string (nullable = true)\n-- OriginalLanguage: string (nullable = true)\n-- ReleaseDate: timestamp (nullable = true)\n-- RunTime: long (nullable = true)\n-- Price: double (nullable = true)\n-- CreatedDate: timestamp (nullable = true)\n-- genre_id: long (nullable = true)\n-- name: string (nullable = true)\n-- quarantined: boolean (nullable = true)\n\n\nData schema:\nroot\n-- Id: long (nullable = true)\n-- Title: string (nullable = true)\n-- Overview: string (nullable = true)\n-- Tagline: string (nullable = true)\n-- Budget: double (nullable = true)\n-- Revenue: double (nullable = true)\n-- ImdbUrl: string (nullable = true)\n-- TmdbUrl: string (nullable = true)\n-- PosterUrl: string (nullable = true)\n-- BackdropUrl: string (nullable = true)\n-- OriginalLanguage: string (nullable = true)\n-- ReleaseDate: timestamp (nullable = true)\n-- RunTime: long (nullable = true)\n-- Price: double (nullable = true)\n-- CreatedDate: timestamp (nullable = true)\n-- genre_id: long (nullable = true)\n-- name: string (nullable = true)\n-- quarantined: boolean (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: A schema mismatch detected when writing to the Delta table (Table ID: bf9b66cb-5e6b-4666-a93c-a17289b2c260).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- Id: long (nullable = true)\n-- Title: string (nullable = true)\n-- Overview: string (nullable = true)\n-- Budget: double (nullable = true)\n-- Revenue: double (nullable = true)\n-- ImdbUrl: string (nullable = true)\n-- TmdbUrl: string (nullable = true)\n-- PosterUrl: string (nullable = true)\n-- BackdropUrl: string (nullable = true)\n-- OriginalLanguage: string (nullable = true)\n-- ReleaseDate: timestamp (nullable = true)\n-- RunTime: long (nullable = true)\n-- Price: double (nullable = true)\n-- CreatedDate: timestamp (nullable = true)\n-- genre_id: long (nullable = true)\n-- name: string (nullable = true)\n-- quarantined: boolean (nullable = true)\n\n\nData schema:\nroot\n-- Id: long (nullable = true)\n-- Title: string (nullable = true)\n-- Overview: string (nullable = true)\n-- Tagline: string (nullable = true)\n-- Budget: double (nullable = true)\n-- Revenue: double (nullable = true)\n-- ImdbUrl: string (nullable = true)\n-- TmdbUrl: string (nullable = true)\n-- PosterUrl: string (nullable = true)\n-- BackdropUrl: string (nullable = true)\n-- OriginalLanguage: string (nullable = true)\n-- ReleaseDate: timestamp (nullable = true)\n-- RunTime: long (nullable = true)\n-- Price: double (nullable = true)\n-- CreatedDate: timestamp (nullable = true)\n-- genre_id: long (nullable = true)\n-- name: string (nullable = true)\n-- quarantined: boolean (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": null,
        "sqlState": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-1569639608351509>, line 45\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;66;03m# write movie table to silver\u001B[39;00m\n\u001B[1;32m     43\u001B[0m     silver_movies_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwriteSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/tables/silver/movies\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 45\u001B[0m process_bronze_to_silver()\n",
        "File \u001B[0;32m<command-1569639608351509>, line 6\u001B[0m, in \u001B[0;36mprocess_bronze_to_silver\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m bronze_df \u001B[38;5;241m=\u001B[39m bronze_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquarantined\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRunTime\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      5\u001B[0m quarantined_df \u001B[38;5;241m=\u001B[39m bronze_df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquarantined\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 6\u001B[0m \u001B[43mquarantined_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbfs:/FileStore/tables/bronze/quarantine/area\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m clean_df \u001B[38;5;241m=\u001B[39m bronze_df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquarantined\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      9\u001B[0m clean_df \u001B[38;5;241m=\u001B[39m clean_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBudget\u001B[39m\u001B[38;5;124m\"\u001B[39m, when(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBudget\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1000000.0\u001B[39m, \u001B[38;5;241m1000000.0\u001B[39m)\u001B[38;5;241m.\u001B[39motherwise(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBudget\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1718\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1716\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1717\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1718\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: A schema mismatch detected when writing to the Delta table (Table ID: bf9b66cb-5e6b-4666-a93c-a17289b2c260).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- Id: long (nullable = true)\n-- Title: string (nullable = true)\n-- Overview: string (nullable = true)\n-- Budget: double (nullable = true)\n-- Revenue: double (nullable = true)\n-- ImdbUrl: string (nullable = true)\n-- TmdbUrl: string (nullable = true)\n-- PosterUrl: string (nullable = true)\n-- BackdropUrl: string (nullable = true)\n-- OriginalLanguage: string (nullable = true)\n-- ReleaseDate: timestamp (nullable = true)\n-- RunTime: long (nullable = true)\n-- Price: double (nullable = true)\n-- CreatedDate: timestamp (nullable = true)\n-- genre_id: long (nullable = true)\n-- name: string (nullable = true)\n-- quarantined: boolean (nullable = true)\n\n\nData schema:\nroot\n-- Id: long (nullable = true)\n-- Title: string (nullable = true)\n-- Overview: string (nullable = true)\n-- Tagline: string (nullable = true)\n-- Budget: double (nullable = true)\n-- Revenue: double (nullable = true)\n-- ImdbUrl: string (nullable = true)\n-- TmdbUrl: string (nullable = true)\n-- PosterUrl: string (nullable = true)\n-- BackdropUrl: string (nullable = true)\n-- OriginalLanguage: string (nullable = true)\n-- ReleaseDate: timestamp (nullable = true)\n-- RunTime: long (nullable = true)\n-- Price: double (nullable = true)\n-- CreatedDate: timestamp (nullable = true)\n-- genre_id: long (nullable = true)\n-- name: string (nullable = true)\n-- quarantined: boolean (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_bronze_to_silver():\n",
    "    bronze_df = read_files_to_bronze()\n",
    "    bronze_df = bronze_df.withColumn(\"quarantined\", col(\"RunTime\") < 0)\n",
    "\n",
    "    quarantined_df = bronze_df.filter(col(\"quarantined\") == True)\n",
    "    quarantined_df.write.format(\"delta\").save(\"dbfs:/FileStore/tables/bronze/quarantine/area\", mode='overwrite')\n",
    "\n",
    "    clean_df = bronze_df.filter(col(\"quarantined\") == False)\n",
    "    clean_df = clean_df.withColumn(\"Budget\", when(col(\"Budget\") < 1000000.0, 1000000.0).otherwise(col(\"Budget\")))\n",
    "\n",
    "    silver_movies_df = clean_df.unionByName(quarantined_df)\n",
    "\n",
    "    # creat genres_lookup_df\n",
    "    genres_lookup_df = (silver_movies_df\n",
    "                    .select(\"genre_id\", \"name\")\n",
    "                    .filter(trim(col(\"name\")) != \"\")\n",
    "                    .distinct()\n",
    "                    )\n",
    "    \n",
    "    genres_lookup_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/silver/genres_lookup\")\n",
    "    \n",
    "    # movie_genres_df\n",
    "    movie_genres_df = silver_movies_df.select(\"Id\", \"genre_id\")\n",
    "    movie_genres_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/silver/movie_genres\")\n",
    "    \n",
    "    # create original_languages_df\n",
    "    original_languages_df = silver_movies_df.select(\"OriginalLanguage\").distinct()\n",
    "    original_languages_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/silver/original_languages\")\n",
    "\n",
    "    # create m:m relationship\n",
    "    silver_movies_df = silver_movies_df.select(\"Id\", \"Title\", \"Overview\", \"Tagline\", \"Budget\", \"Revenue\",\n",
    "                                               \"ImdbUrl\", \"TmdbUrl\", \"PosterUrl\", \"BackdropUrl\",\n",
    "                                               \"OriginalLanguage\", \"ReleaseDate\", \"RunTime\", \"Price\",\n",
    "                                               \"CreatedDate\", \"UpdatedDate\", \"UpdatedBy\", \"CreatedBy\")\n",
    "    \n",
    "    bronze_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"dbfs:/FileStore/tables/bronze/movies\")\n",
    "\n",
    "    # 使用 partitionBy() 和 orderBy() 函数确保每部电影只出现一次\n",
    "    silver_movies_df = silver_movies_df.withColumn(\"row_num\", row_number().over(Window.partitionBy(\"Id\").orderBy(col(\"CreatedDate\").desc())))\n",
    "    silver_movies_df = silver_movies_df.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "    # write movie table to silver\n",
    "    silver_movies_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"dbfs:/FileStore/tables/silver/movies\")\n",
    "\n",
    "process_bronze_to_silver()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Read data from DBFS",
   "widgets": {
    "file_number": {
     "currentValue": "1",
     "nuid": "73f24d2a-1d71-4504-a733-6a12cea50015",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "0",
      "label": "choose the number of the movie files",
      "name": "file_number",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "0",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7"
       ]
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
